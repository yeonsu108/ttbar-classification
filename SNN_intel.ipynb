{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "epoch = 5\n",
    "trainOutput=\"./convSNN_epoch\"+str(epoch)\n",
    "try: os.makedirs(trainOutput, exist_ok=True)\n",
    "except: pass\n",
    "\n",
    "\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from myutils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "nengo_loihi.set_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, *args, activation=True, **kwargs):\n",
    "    # create a Conv2D transform with the given arguments\n",
    "    conv = nengo.Convolution(*args, channels_last=False, **kwargs)\n",
    "\n",
    "    if activation:\n",
    "        # add an ensemble to implement the activation function\n",
    "        layer = nengo.Ensemble(conv.output_shape.size, 1).neurons\n",
    "    else:\n",
    "        # no nonlinearity, so we just use a node\n",
    "        layer = nengo.Node(size_in=conv.output_shape.size)\n",
    "\n",
    "    # connect up the input object to the new layer\n",
    "    nengo.Connection(x, layer, transform=conv)\n",
    "\n",
    "    # print out the shape information for our new layer\n",
    "    print(\"LAYER\")\n",
    "    print(conv.input_shape.shape, \"->\", conv.output_shape.shape)\n",
    "\n",
    "    return layer, conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(y_true, y_pred):\n",
    "    return 100 * tf.metrics.sparse_categorical_accuracy(y_true[:, -1], y_pred[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttbb = \"/homes/yeonsuryou/project_folder/SNN_sample/ttbb_Run2_v6_6j.h5\"\n",
    "ttbj = \"/homes/yeonsuryou/project_folder/SNN_sample/ttbj_Run2_v6_6j.h5\"\n",
    "ttcc = \"/homes/yeonsuryou/project_folder/SNN_sample/ttcc_Run2_v6_6j.h5\"\n",
    "ttlf = \"/homes/yeonsuryou/project_folder/SNN_sample/ttlf_Run2_v6_6j.h5\"\n",
    "\n",
    "#set up training data\n",
    "pd_ttbb = pd.read_hdf(ttbb)\n",
    "pd_ttbj = pd.read_hdf(ttbj).sample(n=len(pd_ttbb))\n",
    "pd_ttcc = pd.read_hdf(ttcc).sample(n=len(pd_ttbb))\n",
    "pd_ttlf = pd.read_hdf(ttlf).sample(n=len(pd_ttbb))\n",
    "\n",
    "pd_ttbj[\"event_category\"] = 1\n",
    "pd_ttcc[\"event_category\"] = 2\n",
    "pd_ttlf[\"event_category\"] = 3\n",
    "\n",
    "# merge data and reset index\n",
    "pd_data = pd.concat([pd_ttlf, pd_ttbb, pd_ttcc, pd_ttbj], ignore_index=True)\n",
    "data = pd_data.sample(frac=1).reset_index(drop=True)\n",
    "# print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 48\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# event_var = ['nbjets_m', 'ncjets_m', 'ngoodjets', 'St', 'Ht', 'lepton_pt', 'lepton_eta', 'lepton_phi', 'lepton_m', 'lepton_e', 'MET', 'MET_phi', 'nulep_pt', \n",
    "#              \"dR12\", \"dR13\", \"dR14\", \"dR15\", \"dR16\", \"dR23\", \"dR24\", \"dR25\", \"dR26\", \"dR34\", \"dR35\", \"dR36\", \"dR45\", \"dR46\", \"dR56\", \n",
    "#              \"dEta12\", \"dEta13\", \"dEta14\", \"dEta15\", \"dEta16\", \"dEta23\", \"dEta24\", \"dEta25\", \"dEta26\", \"dEta34\", \"dEta35\", \"dEta36\", \"dEta45\", \"dEta46\", \"dEta56\", \n",
    "#              \"dPhi12\", \"dPhi13\", \"dPhi14\", \"dPhi15\", \"dPhi16\", \"dPhi23\", \"dPhi24\", \"dPhi25\", \"dPhi26\", \"dPhi34\", \"dPhi35\", \"dPhi36\", \"dPhi45\", \"dPhi46\", \"dPhi56\", \n",
    "#              \"invm12\", \"invm13\", \"invm14\", \"invm15\", \"invm16\", \"invm23\", \"invm24\", \"invm25\", \"invm26\", \"invm34\", \"invm35\", \"invm36\", \"invm45\", \"invm46\", \"invm56\", \n",
    "#              \"dRnulep12\", \"dRnulep13\", \"dRnulep14\", \"dRnulep15\", \"dRnulep16\", \"dRnulep23\", \"dRnulep24\", \"dRnulep25\", \"dRnulep26\", \"dRnulep34\", \"dRnulep35\", \"dRnulep36\", \"dRnulep45\", \"dRnulep46\", \"dRnulep56\"]\n",
    "# jet_var = [\"jet1_pt\", \"jet1_eta\", \"jet1_e\", \"jet1_m\", \"jet1_btag\", \"jet1_cvsb\", \"jet1_cvsl\", \"dRlep1\", \"dRnu1\", \"dRnulep1\", \"invmlep1\", \"invmnu1\", \n",
    "#            \"jet2_pt\", \"jet2_eta\", \"jet2_e\", \"jet2_m\", \"jet2_btag\", \"jet2_cvsb\", \"jet2_cvsl\", \"dRlep2\", \"dRnu2\", \"dRnulep2\", \"invmlep2\", \"invmnu2\", \n",
    "#            \"jet3_pt\", \"jet3_eta\", \"jet3_e\", \"jet3_m\", \"jet3_btag\", \"jet3_cvsb\", \"jet3_cvsl\", \"dRlep3\", \"dRnu3\", \"dRnulep3\", \"invmlep3\", \"invmnu3\", \n",
    "#            \"jet4_pt\", \"jet4_eta\", \"jet4_e\", \"jet4_m\", \"jet4_btag\", \"jet4_cvsb\", \"jet4_cvsl\", \"dRlep4\", \"dRnu4\", \"dRnulep4\", \"invmlep4\", \"invmnu4\", \n",
    "#            \"jet5_pt\", \"jet5_eta\", \"jet5_e\", \"jet5_m\", \"jet5_btag\", \"jet5_cvsb\", \"jet5_cvsl\", \"dRlep5\", \"dRnu5\", \"dRnulep5\", \"invmlep5\", \"invmnu5\", \n",
    "#            \"jet6_pt\", \"jet6_eta\", \"jet6_e\", \"jet6_m\", \"jet6_btag\", \"jet6_cvsb\", \"jet6_cvsl\", \"dRlep6\", \"dRnu6\", \"dRnulep6\", \"invmlep6\", \"invmnu6\"]\n",
    "event_var = ['nbjets_m', 'ncjets_m', 'ngoodjets','St', 'Ht', 'lepton_pt', 'lepton_eta', 'lepton_phi', 'lepton_m', 'MET', 'MET_phi', 'nulep_pt',\n",
    "             \"dEta12\", \"dEta13\", \"dEta14\", \"dEta15\", \"dEta16\", \"dEta23\", \"dEta24\", \"dEta25\", \"dEta26\", \"dEta34\", \"dEta35\", \"dEta36\", \"dEta45\", \"dEta46\", \"dEta56\", \n",
    "             \"dPhi12\", \"dPhi13\", \"dPhi14\", \"dPhi15\", \"dPhi16\", \"dPhi23\", \"dPhi24\", \"dPhi25\", \"dPhi26\", \"dPhi34\", \"dPhi35\", \"dPhi36\", \"dPhi45\", \"dPhi46\", \"dPhi56\", \n",
    "             \"invm12\", \"invm13\", \"invm14\", \"invm15\", \"invm16\", \"invm23\", \"invm24\", \"invm25\", \"invm26\", \"invm34\", \"invm35\", \"invm36\", \"invm45\", \"invm46\", \"invm56\", \n",
    "             \"dRnulep12\", \"dRnulep13\", \"dRnulep14\", \"dRnulep15\", \"dRnulep16\", \"dRnulep23\", \"dRnulep24\", \"dRnulep25\", \"dRnulep26\", \"dRnulep34\", \"dRnulep35\", \"dRnulep36\", \"dRnulep45\", \"dRnulep46\", \"dRnulep56\"]\n",
    "jet_var = [\"jet1_pt\", \"jet1_eta\", \"jet1_m\", \"jet1_btag\", \"jet1_cvsl\", \"dRlep1\", \"dRnu1\", \"invmlep1\", \n",
    "           \"jet2_pt\", \"jet2_eta\", \"jet2_m\", \"jet2_btag\", \"jet2_cvsl\", \"dRlep2\", \"dRnu2\", \"invmlep2\", \n",
    "           \"jet3_pt\", \"jet3_eta\", \"jet3_m\", \"jet3_btag\", \"jet3_cvsl\", \"dRlep3\", \"dRnu3\", \"invmlep3\", \n",
    "           \"jet4_pt\", \"jet4_eta\", \"jet4_m\", \"jet4_btag\", \"jet4_cvsl\", \"dRlep4\", \"dRnu4\", \"invmlep4\", \n",
    "           \"jet5_pt\", \"jet5_eta\", \"jet5_m\", \"jet5_btag\", \"jet5_cvsl\", \"dRlep5\", \"dRnu5\", \"invmlep5\", \n",
    "           \"jet6_pt\", \"jet6_eta\", \"jet6_m\", \"jet6_btag\", \"jet6_cvsl\", \"dRlep6\", \"dRnu6\", \"invmlep6\"]\n",
    "\n",
    "variables = event_var + jet_var\n",
    "print (len(event_var), len(jet_var))\n",
    "\n",
    "class_names = [\"ttbb\", \"ttbj\", \"ttcc\", \"ttlf\"]\n",
    "\n",
    "nVariables, nClass = len(variables), len(class_names)\n",
    "print (nVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(data.filter(items = variables))\n",
    "train_out  = np.array(data.filter(items = ['event_category']))\n",
    "\n",
    "numbertr=len(train_out)\n",
    "#trainlen = int(numbertr * 0.8)\n",
    "trainlen = int(numbertr - 500)\n",
    "\n",
    "train_out = train_out.reshape( (numbertr, 1) )\n",
    "\n",
    "valid_data=train_data[trainlen:,0::]\n",
    "valid_data_out=train_out[trainlen:]\n",
    "\n",
    "train_data=train_data[:trainlen,0::]\n",
    "train_data_out=train_out[:trainlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER\n",
      "(1, 10, 12) -> (1, 10, 12)\n",
      "LAYER\n",
      "(1, 10, 12) -> (16, 8, 10)\n",
      "LAYER\n",
      "(16, 8, 10) -> (32, 6, 8)\n",
      "LAYER\n",
      "(1, 10, 12) -> (1, 10, 12)\n",
      "LAYER\n",
      "(1, 10, 12) -> (16, 8, 10)\n",
      "LAYER\n",
      "(16, 8, 10) -> (32, 6, 8)\n"
     ]
    }
   ],
   "source": [
    "dt = 0.001  # simulation timestep\n",
    "presentation_time = 0.1  # input presentation time\n",
    "max_rate = 100  # neuron firing rates\n",
    "# neuron spike amplitude (scaled so that the overall output is ~1)\n",
    "amp = 1 / max_rate\n",
    "# input image shape\n",
    "input_shape = (1, 10, 12)\n",
    "n_parallel = 2  # number of parallel network repetitions\n",
    "minibatch_size = 200\n",
    "\n",
    "with nengo.Network(seed=0) as net:\n",
    "    # set up the default parameters for ensembles/connections\n",
    "    nengo_loihi.add_params(net)\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    #neuron_type = nengo.SpikingRectifiedLinear(amplitude=amp)\n",
    "    neuron_type = nengo.LIF(tau_rc=0.02, tau_ref=0.001, amplitude=amp)\n",
    "    #neuron_type = nengo.AdaptiveLIF(amplitude=amp)\n",
    "    #neuron_type = nengo.Izhikevich()\n",
    "    #neuron_type = nengo_loihi.neurons.LoihiLIF(tau_rc=0.02, tau_ref=0.001, amplitude=amp)\n",
    "    net.config[nengo.Ensemble].neuron_type = neuron_type\n",
    "\n",
    "    # the input node that will be used to feed in input images\n",
    "    inp = nengo.Node(\n",
    "        nengo.processes.PresentInput(valid_data, presentation_time), size_out=nVariables\n",
    "    )\n",
    "\n",
    "    # the output node provides the 10-dimensional classification\n",
    "    out = nengo.Node(size_in=nClass)\n",
    "\n",
    "    # build parallel copies of the network\n",
    "    for _ in range(n_parallel):\n",
    "        layer, conv = conv_layer(inp, 1, input_shape, kernel_size=(1, 1), init=np.ones((1, 1, 1, 1)))\n",
    "        # first layer is off-chip to translate the images into spikes\n",
    "        net.config[layer.ensemble].on_chip = False\n",
    "        #layer, conv = conv_layer(layer, 2, conv.output_shape, strides=(1, 1))\n",
    "        layer, conv = conv_layer(layer, 16, conv.output_shape, strides=(1, 1))\n",
    "        layer, conv = conv_layer(layer, 32, conv.output_shape, strides=(1, 1))\n",
    "        \n",
    "#         dense_layer = nengo.Ensemble(n_neurons=100, dimensions=1, neuron_type=neuron_type, label=\"Layer 1\").neurons\n",
    "#         nengo.Connection(layer, dense_layer, transform=nengo_dl.dists.Glorot())\n",
    "\n",
    "#         nengo.Connection(dense_layer, out, transform=nengo_dl.dists.Glorot())\n",
    "    nengo.Connection(layer, out, transform=nengo_dl.dists.Glorot())\n",
    "\n",
    "    out_p = nengo.Probe(out, label=\"out_p\")\n",
    "    out_p_filt = nengo.Probe(out, synapse=nengo.Alpha(0.01), label=\"out_p_filt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set train data\n",
    "train_data = train_data.reshape(-1, 1, nVariables)\n",
    "valid_data = valid_data.reshape(-1, 1, nVariables)\n",
    "train_data_out = train_data_out.reshape(-1, 1, 1)\n",
    "valid_data_out = valid_data_out.reshape(-1, 1, 1)\n",
    "\n",
    "train_data = {inp: train_data, out_p: train_data_out}\n",
    "\n",
    "# for the test data evaluation we'll be running the network over time\n",
    "# using spiking neurons, so we need to repeat the input/target data\n",
    "# for a number of timesteps (based on the presentation_time)\n",
    "test_data = {\n",
    "    inp: np.tile(valid_data, (1, int(presentation_time / dt), 1)),\n",
    "    out_p_filt: np.tile(valid_data_out, (1, int(presentation_time / dt), 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/yeonsuryou/miniconda3/envs/loihi/lib/python3.5/site-packages/nengo_dl/simulator.py:461: UserWarning: No GPU support detected. See https://www.nengo.ai/nengo-dl/installation.html#installing-tensorflow for instructions on setting up TensorFlow with GPU support.\n",
      "  \"No GPU support detected. See \"\n",
      "/homes/yeonsuryou/miniconda3/envs/loihi/lib/python3.5/site-packages/nengo_dl/transform_builders.py:53: UserWarning: TensorFlow does not support convolution with channels_last=False on the CPU; inputs will be transformed to channels_last=True\n",
      "  UserWarning,\n",
      "/homes/yeonsuryou/miniconda3/envs/loihi/lib/python3.5/site-packages/nengo_dl/simulator.py:1930: UserWarning: Number of elements in input data (500) is not evenly divisible by Simulator.minibatch_size (200); input data will be truncated.\n",
      "  % (data_batch, self.minibatch_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy before training: 30.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/yeonsuryou/miniconda3/envs/loihi/lib/python3.5/site-packages/nengo_dl/simulator.py:1773: UserWarning: Number of elements (1) in ['ndarray'] does not match number of Probes (2); consider using an explicit input dictionary in this case, so that the assignment of data to objects is unambiguous.\n",
      "  len(objects),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3671/5374 [===================>..........] - ETA: 8:59 - loss: 1.0041 - out_p_loss: 1.0041 - out_p_sparse_categorical_accuracy: 0.5472"
     ]
    }
   ],
   "source": [
    "do_training = True\n",
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_size, seed=0, progress_bar=False) as sim:\n",
    "    if do_training:\n",
    "        sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "        print(\"accuracy before training: %.2f%%\" %\n",
    "                sim.evaluate(test_data[inp], {out_p_filt: test_data[out_p_filt]}, verbose=0)[\"loss\"])\n",
    "        \n",
    "        training_start = time.time()\n",
    "        # run training\n",
    "        sim.compile(\n",
    "                optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "                loss={out_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "                metrics={out_p: tf.metrics.sparse_categorical_accuracy},\n",
    "        )\n",
    "        sim.fit(train_data[inp], train_data[out_p], epochs=epoch)\n",
    "        \n",
    "        print (\"training time: \", (time.time()-training_start))\n",
    "\n",
    "        sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "        print(\"accuracy after training: %.2f%%\" %\n",
    "                sim.evaluate(test_data[inp], {out_p_filt: test_data[out_p_filt]}, verbose=0)[\"loss\"])\n",
    "\n",
    "        sim.save_params(trainOutput+\"/model\")\n",
    "    else:\n",
    "        sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "        print(\"accuracy before training: %.2f%%\" %\n",
    "                sim.evaluate(test_data[inp], {out_p_filt: test_data[out_p_filt]}, verbose=0)[\"loss\"])\n",
    "        sim.load_params(trainOutput+\"/model\")\n",
    "        print(\"parameters loaded\")\n",
    "        sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "        print(\"accuracy after training: %.2f%%\" %\n",
    "                sim.evaluate(test_data[inp], {out_p_filt: test_data[out_p_filt]}, verbose=0)[\"loss\"])\n",
    "\n",
    "    # store trained parameters back into the network\n",
    "    sim.freeze_params(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conn in net.all_connections:\n",
    "    conn.synapse = 0.005\n",
    "\n",
    "if do_training:\n",
    "    with nengo_dl.Simulator(net, minibatch_size=minibatch_size, progress_bar=False) as sim:\n",
    "        sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "        print(\"accuracy w/ synapse: %.2f%%\" %\n",
    "                sim.evaluate(test_data[inp], {out_p_filt: test_data[out_p_filt]}, verbose=0)[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_presentations = 500\n",
    "start = time.time()\n",
    "# with nengo_loihi.Simulator(net, dt=dt, precompute=False, progress_bar=False) as sim:\n",
    "with nengo_loihi.Simulator(net, dt=dt, precompute=False, progress_bar=False, target=\"sim\") as sim:\n",
    "    # if running on Loihi, increase the max input spikes per step\n",
    "    if 'loihi' in sim.sims:\n",
    "        sim.sims['loihi'].snip_max_spikes_per_step = 100\n",
    "#    class_names = [\"ttlf\", \"ttb\", \"ttbb\", \"ttc\", \"ttcc\"]\n",
    "\n",
    "    # run the simulation on Loihi\n",
    "    sim.run(n_presentations * presentation_time)\n",
    "\n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"hardware time: \", (end-start))\n",
    "    # check classification error\n",
    "    step = int(presentation_time / dt)\n",
    "    output = sim.data[out_p_filt][step - 1::step]\n",
    "    #print (\"output\", output)\n",
    "    error_percentage = 100 * np.mean(\n",
    "        np.argmax(output, axis=-1) != test_data[out_p_filt][:n_presentations, -1, 0]\n",
    "    )\n",
    "    acc = 100 * np.mean(\n",
    "        np.argmax(output, axis=-1) == test_data[out_p_filt][:n_presentations, -1, 0]\n",
    "    )\n",
    "    \n",
    "    predicted = np.argmax(output, axis=-1)\n",
    "    correct = test_data[out_p_filt][:n_presentations, -1, 0]\n",
    "\n",
    "    predicted = np.array(predicted, dtype=int)\n",
    "    correct = np.array(correct, dtype=int)\n",
    "\n",
    "    print (len(predicted))\n",
    "    print (len(correct))\n",
    "    print(\"Predicted labels: \", predicted[:500])\n",
    "    print(\"Correct labels: \", correct[:500])\n",
    "    print(\"loihi error: %.2f%%\" % error_percentage)\n",
    "    print(\"loihi acc: %.2f%%\" % acc)\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(correct, predicted, classes=class_names,\n",
    "                    title='Confusion matrix, without normalization, acc=%.2f'%acc, savename=trainOutput+\"/confusion_matrix.png\", show=False)\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plot_confusion_matrix(correct, predicted, classes=class_names, normalize=True,\n",
    "                    title='Normalized confusion matrix, acc=%.2f'%acc, savename=trainOutput+\"/norm_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    n_plots = 5\n",
    "    correct = test_data[out_p_filt][n_plots*i:n_plots*(i+1), -1, 0]\n",
    "    correct_str = \"             \"\n",
    "    for j in correct:\n",
    "        correct_str += class_names[j] + \"           \"\n",
    "    correct = \"\".join(map(str, correct))\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(sim.trange()[n_plots*i*step:n_plots*(i+1)*step], tf.nn.softmax(sim.data[out_p_filt][n_plots*i*step:n_plots*(i+1)*step]))\n",
    "    for j in sim.trange()[n_plots*i*step:n_plots*(i+1)*step+1:100]:\n",
    "        plt.axvline(x=j, color='r', linestyle='--', linewidth=1)\n",
    "    plt.legend(class_names, loc=\"upper right\", bbox_to_anchor=(1.2, 0.99))\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.title(correct_str)\n",
    "\n",
    "    plt.savefig(trainOutput+\"/label_\"+correct+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
